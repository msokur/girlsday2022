{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"girlday.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Datenanalyse: Verbrechen in San Francisco\n",
        "\n",
        "Für das folgende Tutorial zur Datenanalyse nutzen wir die Programmiersprache **Python**.\n",
        "Python ist eine gutlesbare Programmiersprache und wird typischerweise in der Datenanalyse genutzt.\n",
        "\n",
        "Auch die Programmiersprache R wird häufig zur Datenverarbeitung genutzt, sie unterscheidet sich allerdings stark von Python.\n",
        "\n",
        "### 1. Importieren von Bibliotheken\n",
        "\n",
        "Zuerst importieren wir einige Standart-Bibliotheken für die Datenanalyse. Sie stellen viele Funktionalitäten bereit, um nicht alles von Grund auf programmieren zu müssen.\n",
        "\n",
        "Methoden der Bibliotheken können aufgerufen über den zugewiesenen Name der Bibliothek z.B. \"pd\", dahinter ein Punkt und im Anschluss den Mehtodenname z.B. \"pd.read_csv(...)\" um eine Csv-Datein einzulesen."
      ],
      "metadata": {
        "id": "0BGyRPOUR4iH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_-x75GCcCZ4"
      },
      "outputs": [],
      "source": [
        "# Mit der Raute schreiben wir einen Kommentar, es ist kein Programmcode und wird daher nicht ausgeführt\n",
        "\n",
        "# numpy und pandas werden zur Datenverarbeitung genutzt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# matplotlib wird zur Visualisierung der Daten genutzt\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# sklearn enthält viele Methoden um ML auf Datensätze anzuwenden, hier benutzen wir SVM (Support Vector Machine)\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Laden des Datensatzes"
      ],
      "metadata": {
        "id": "Abzy1CD3SWhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir laden den Datensatz, der als CSV-Datei (ähnlich einer Excel-Tabelle) vorliegt, als Pandas Dataframe um mit den Daten arbeiten zu können.\n",
        "# Wir nennen den Datensatz \"crime_dataframe\", unter diesem Namen können wir im Weiteren auf ihn zugreifen.\n",
        "crime_dataframe = pd.read_csv('https://cloudold.scadsai.uni-leipzig.de/index.php/s/At9fMs9X48EJ3Jr/download/girlsday-sanfrancrimes.csv')"
      ],
      "metadata": {
        "id": "FoG9HkPnc-TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Ersten Überblick über den Datensatz verschaffen"
      ],
      "metadata": {
        "id": "9w5WS9E8kSsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wie viele Einträge hat der Datensatz?\n",
        "# Die Methode len() liefert uns die Länge des Datensatzes\n",
        "number_of_entries = len(crime_dataframe)\n",
        "# mit print(\"...\") können wir eine Ausgabe erzeugen\n",
        "print(f\"Dieser Datensatz hat ganze {number_of_entries} Einträge!\")"
      ],
      "metadata": {
        "id": "fJnwTQOjkUCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In welche Kategorien ist der Datensatz aufgeteilt?\n",
        "# Die Methode columns() liefert um die Spalten-Namen des Datensatzes\n",
        "crime_columns = list(crime_dataframe.columns)\n",
        "print(f\"Dieser Datensatz hat die folgenden Spalten-Namen: \\n {crime_columns}.\")"
      ],
      "metadata": {
        "id": "NQeK1UYhSl91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Um einschätzen zu können welche Werte wir für die einzelnen Kategorien zu erwarten haben\n",
        "# betrachten wir die ersten Zeilen des Datensatzes\n",
        "# Den \"Kopf\" bzw. die ersten 5 Zeilen des Datensatz können wir mit der Methode \"head()\" anzeigen\n",
        "print(crime_dataframe.head())"
      ],
      "metadata": {
        "id": "gU3PqS1YSuDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Datenanlyse mit Diagrammen\n",
        "\n",
        "Mit Diagrammen können wir uns noch einen besseren Überblick über die Daten verschaffen und erste Schlüsse ziehen.\n",
        "\n",
        "#### 4.1 Das Histogramm\n",
        "Wir wollen herausfinden welche Verbrechen am häufigsten begangen wurden und betrachten dazu die Kategorie 'Catagory'.\n",
        "\n",
        "Ein Histogramm gibt die absolute Häufigkeitsverteilung an und ist deshalb gut geeignet."
      ],
      "metadata": {
        "id": "i69uVL9XS4b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Welche unterschiedlichen Werte gibt es in der Spalte 'Categorie'?\n",
        "categories = crime_dataframe['Category'].value_counts().index\n",
        "print(f\"In der Spalte 'Categorie' gibt es die verschiedenen Kategorien: \\n {categories.values} \\n\")\n",
        "print(f\"Es sind insgesamt {len(categories.values)} verschiedene Kategorien. \\n\")\n",
        "\n",
        "# Wie häufig kommen die einzelnen Kategorien vor?\n",
        "counts = crime_dataframe['Category'].value_counts().values\n",
        "print(f\"Die einzelnen Kategorien kommen in dieser absoluten Häufigkeit vor: \\n {counts} \\n\")\n",
        "\n",
        "# Diese beiden Felder an Daten setzen wir jetzt zusammen in einem Plot\n",
        "# Mit der Methode \"bar\" der Bibliothek plt\n",
        "# Der Methode übergeben wir die beiden Felder und stellen die Weite der einzelnen Balken ein\n",
        "plt.bar(categories, counts, width=0.7)\n",
        "\n",
        "# Formatierung des Plots\n",
        "plt.xticks(rotation = 90)\n",
        "fig = plt.gcf() \n",
        "fig.set_size_inches(11,8)\n",
        "\n",
        "# Anzeigen des Plots\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lYpWcFRFS8UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Als nächstes möchten wir wissen, ob es Wochentage gibt an denen besonders viele Verbrechen beganngen werden.\n",
        "\n",
        "Dazu erstellen wir ein weiteres Histogramm und wählen diesmal die Spalte 'DayOfWeek'.\n",
        "\n",
        "Der Code ist nahe zu identisch, an welchen Stellen muss er ausgetauscht werden?\n",
        "Tausche den Code so aus, das er ein Histogramm für die Spalte 'DayOfWeek' ausgibt."
      ],
      "metadata": {
        "id": "Z7JCXg-FTDdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Welche unterschiedlichen Werte gibt es in der Spalte?\n",
        "categories = crime_dataframe['Category'].value_counts().index\n",
        "print(f\"In der Spalte gibt es die verschiedenen Kategorien: \\n {categories.values} \\n\")\n",
        "\n",
        "# Wie häufig kommen die einzelnen Kategorien vor?\n",
        "counts = crime_dataframe['Category'].value_counts().values\n",
        "print(f\"Die einzelnen Kategorien kommen in dieser absoluten Häufigkeit vor: \\n {counts} \\n\")\n",
        "\n",
        "# Diese beidem Felder an Daten setzen wir jetzt zusammen in einem Plot\n",
        "# Mit der Methode bar der Bibliothek plt\n",
        "# Der Methode übergeben wir die beiden Felder und stellen die Weite der Balken ein\n",
        "plt.bar(categories, counts, width=0.7)\n",
        "\n",
        "# Formatierung des Plots\n",
        "plt.xticks(rotation = 90)\n",
        "fig = plt.gcf() \n",
        "fig.set_size_inches(11,8)\n",
        "\n",
        "# Anzeigen des Plots\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Fhw1t62kTGed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im nächsten Schritt spezialisieren wir uns auf eine Kategorie, auf \n",
        "Autofahren unter Einfluss von Alkohol und Drogen (engl. 'DRIVING UNDER THE INFLUENCE').\n",
        "\n",
        "Wir wollen wissen:\n",
        "An welchem Tag wird besonders oft alkoholisiert Auto gefahren?\n",
        "\n",
        "Dazu kreieren wir ein Subset des Datensatzes und benutzten nur die Daten für die gilt, dass die 'Category' einen bestimmten Wert hat, hier 'DRIVING UNDER THE INFLUENCE' ist.\n",
        "\n",
        "Probiere auch gerne mal eine der anderen Kategorien aus, z.B. 'DRUNKENNESS', 'ROBBERY', 'GAMBLING' oder 'VANDALISM'.\n",
        "\n",
        "Außerdem ändern wir die Farbe in grün und die Balkenweite auf einen kleineren Wert. Probiere gerne mal andere Farben wie:'yellow', 'orange', 'purple', 'red', 'black', 'magenta', 'cyan' und neue Werte für die Balkenweite ('width') aus."
      ],
      "metadata": {
        "id": "ejsCgL_xTKHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen des Subsets\n",
        "subset = crime_dataframe[crime_dataframe['Category']== 'DRIVING UNDER THE INFLUENCE']\n",
        "\n",
        "# Anzeigen der ersten Spalten des Subsets\n",
        "print(subset.head())\n",
        "\n",
        "# Erstellen des Plots\n",
        "categories = subset['DayOfWeek'].value_counts().index\n",
        "counts = subset['DayOfWeek'].value_counts().values\n",
        "\n",
        "plt.bar(categories, counts, width=0.2, color=\"green\")\n",
        "plt.xticks(rotation = 90)\n",
        "fig = plt.gcf() \n",
        "fig.set_size_inches(8,6)\n",
        "fig.show() "
      ],
      "metadata": {
        "id": "9ZOjJm4PTMVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Die Kreuztabelle (Cross Table)\n",
        "\n",
        "\n",
        "Um Zusammenhänge zwischen mehreren Spalten zu erkennen, ist es nützlich eine Kreuztabelle zu erstellen.\n",
        "\n",
        "Sie stellt 2 Spalten gegenüber und gibt die absoluten Häufigkeiten der Merkmals-Kombinationen an.\n",
        "\n",
        "Probiere jeweils zwei beliebige Spalten aus und bringe sie über die Kreuztabelle in Verbindung, \n",
        "z.B. 'Category' und 'Resolution' um herauszufinden mit welcher Strafe man bei welchem Verbrechen zu rechnen hat.\n",
        "\n",
        "Falls du vergessen hast, welche Spalten es gab, erinnerst du dich, wie wir uns diese anzeigen konnten?"
      ],
      "metadata": {
        "id": "Z8KwKX5GTZ3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(crime_dataframe['Category'], crime_dataframe['Resolution'])"
      ],
      "metadata": {
        "id": "czcge6EkTZqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Vorhersage-Richter:in\n",
        "Zum Abschluss stellen wir uns ein Szenario vor, indem ein Computerprogramm anhand von \"alten Urteilen\" entscheidet, welche Strafe verhängt werden soll. Wir generieren uns also eine:n Vorhersage-Richter:in.\n",
        "\n",
        "Dafür nutzen wir das häufig genutzte **Machine Learning** Verfahren **Support Vector Machine** (SVM), das Objekte in Klassen teilt um damit eine Vorhersage über bisher unbekannte Objekte zutreffen. Keine Sorge, wenn du noch nicht so viel verstehst, das schauen wir uns gleich noch mal genauer an.\n",
        "\n",
        "In unserem Beispiel wollen wir mit den Spalte \"Category\" (Delikt) und \"PdDistrict\" eine Vorhersage darüber machen, was für eine Strafe die Personen erwartet (Spalte \"Resolution\").\n",
        "SVM ist ein überwacht lernendes Verfahren, das bedeutet, dass wir für eine Eingabe (in diesem Fall die Spalten \"Category\", \"PdDistrict\") ein Zielwert, also eine bestimmte Strafe vorgeben müssen, zu finden in der Spalte \"Resolution\".\n",
        "Die SVM lernt dann aus diesen gegebenen Beispielen und kann Vorhersagen für neue Eingaben treffen, also welche Strafe in diesem Fall verhängt wird.\n",
        "\n",
        "Im ersten Schritt teilen wir unseren Datensatz in unsere **Eingabe**, in der Regel mit **X** bezeichnet, und den **Zielwert**, in der Regel mit **y** bezeichnet.\n",
        "Im zweiten Schritt teilen wir diese weiter, in **Trainingsdatensatz**, mit diesem lernen wir die SVM, und in einen **Testdatensatz**, an dem wir dann die entstandene SVM testen können.\n",
        "\n",
        "Am besten funktioniert das Verfahren, wenn wir ihm nur Zahlen übergeben, in den Spalten \"Category\" und \"PdDistrict\" haben wir es aber mit Zeichenketten(string) zu tun, mit denen die SVM nicht so gut arbeiten kann. \n",
        "Wir nutzen deshalb einen Trick um die Zeichenketten in Zahlen umzuwandeln, genannt **One-Hot-Encoding**.\n",
        "Dabei erzeugen wir für jede Möglichkeit des Deliktes einen Vektor (eine Reihe von Zahlen), der die selbe Länge hat wie die Anzahl der verschiedenen Delikte. Der Vektor besteht an allen Stellen aus Nullen, nur an einer Stelle steht eine 1. Diese Stelle ist charakterisch für das jeweilige Delikt.\n",
        "\n",
        "\n",
        "Hier ein kleines Beispiel:\n",
        "- Es gibt die verschiedene Delikte: Verkehrsdelikt, Vandalismus, Diebstahl\n",
        "\n",
        "→ Anzahl der verschiedenen Delikte ist: 3\n",
        "\n",
        "→ wir erzeugen also einen Vektor der Länge 3\n",
        "\n",
        "→ für die verschiedenen Delikte setzen wir an verschiedenen Stellen die 1 im Vektor, an den restlichen Stellen setzen wir Nullen\n",
        "\n",
        "→ für Verkehrsdelikt erzeugen wir den Vektor: 100\n",
        "\n",
        "→ für Vandalismus erzeugen wir den Vektor: 010\n",
        "\n",
        "→ für Diebstahl erzeugen wir den Vektor: 001"
      ],
      "metadata": {
        "id": "lhIITPptTmn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir definieren den Zielwert y\n",
        "y = crime_dataframe['Resolution']\n",
        "\n",
        "# One-Hot-Vektoren für die Spalte 'Category' erstellen\n",
        "X_category = pd.get_dummies(crime_dataframe['Category'], columns=['Category'])\n",
        "# One Hot-Vektoren für die Spalte 'PdDistrict' erstellen\n",
        "X_district = pd.get_dummies(crime_dataframe['PdDistrict'], columns=['PdDistrict'])\n",
        "# Verbinden der One-Hot-Vektoren beider Spalten zu einem DataFrame\n",
        "X = pd.concat([X_category, X_district], axis= 1)\n",
        "\n",
        "# Wir teilen unseren Datensatz auf in Trainings- und Testdatensatz\n",
        "# Dabei nutzen wir 80% (0.8) des Datensatzes zum Testen und die anderen 20% zu Trainieren\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "print(f\"Unser Trainingsdatensatz umfasst {len(X_train)} Einträge \\n\")\n",
        "print(f\"Unser Testdatensatz umfasst {len(X_test)} Einträge \\n\")"
      ],
      "metadata": {
        "id": "660ODYAylRno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72be2723-510b-4d7d-8f2e-242c8e10214d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unser Trainingsdatensatz umfasst 175609 Einträge \n",
            "\n",
            "Unser Testdatensatz umfasst 702440 Einträge \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im nächsten Schritt würden wir unsere SVM trainineren. Allerdings kann das sehr lange dauern bei dieser großen Datenmenge. Deshalb haben wir das für euch schon im Vorhinein gemacht und wir laden das Modell einfach nur."
      ],
      "metadata": {
        "id": "gl9BCvVpkrgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier wird unser Modell trainiert\n",
        "'''\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train.to_numpy(), y_train.to_numpy())\n",
        "\n",
        "# Speichern des Modells als binary file\n",
        "with open('pickle.pkl', 'wb') as file:\n",
        "      \n",
        "    # A new file will be created\n",
        "    pickle.dump(clf, file)\n",
        "'''"
      ],
      "metadata": {
        "id": "Ly1a0lFi45Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unter dieser Url liegt das vortrainierte Modell\n",
        "url = \"https://cloudold.scadsai.uni-leipzig.de/index.php/s/b9HkYJ9eD6RNRQx/download/pickle.pkl\"\n",
        "\n",
        "# Herunterladen des Modells\n",
        "with urlopen(url) as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Speichern des Modells\n",
        "with open('pickle.pkl', 'wb') as download:\n",
        "    download.write(content)\n",
        "\n",
        "# Laden des Modells\n",
        "with open('pickle.pkl', 'rb') as file:\n",
        "      clf = pickle.load(file)"
      ],
      "metadata": {
        "id": "8cccDs7DI35R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir können nun anhand unseres Testdatensatzes überprüfen, wie gut unser Modell die Strafen vorhersagen kann.\n",
        "\n",
        "Dazu erstellen wir eine Vorhersage für die Testdaten. Das Modell bekommt in diesem Fall nur die Kategorie des Verbrechens übergeben und erzeugt damit eine Vorhersage der Strafe.\n",
        "\n",
        "Die vorhergesagte Strafe können wir dann mit der Strafe, die im Datensatz steht, vergleichen.\n",
        "\n",
        "Um alle einzelnen Verbrechen im Testdatensatz zu überprüfen, nutzen wir eine **For-Schleife**. Sie führt den gleichen Programmcode für alle im Testdatensatz enthaltenen Elemente einmal aus. Dabei zählen wir wie oft eine Vorhersage falsch oder richtig war. War eine Vorhersage falsch, erhöhen wir den Zähler für die falschen Vorhersagen um eins. War eine Vorhersage richtig, erhöhen wir den Zähler für die richtigen Vorhersagen um eins.\n",
        "\n",
        "Zum Schluss können wir mit einer Prozentzahl die Genauigkeit des Modells angeben, die Accurcay. Sie beschreibt wie oft das Modell richtig lag im Verhältnis zu allen Vorhersagen."
      ],
      "metadata": {
        "id": "C4I3_DR6a2mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir nutzen nur einen kleinen Teil (0.1%) des Testdatensatzes, da die Berechnung sonst zu lange dauert\n",
        "X_test = X_test.sample(frac =0.001)\n",
        "\n",
        "print(f\"Unser Testdatensatz besteht jetzt nur noch aus {len(X_test)} Einträgen\")"
      ],
      "metadata": {
        "id": "poYkpMrwjeGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen der Vorhersage auf dem Testdatensatz\n",
        "pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "h4uNjNRdj2F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir zählen mit, wie oft unser Modell richtig oder falsch lag\n",
        "# Dazu erzeugen wir zwei Zähler und setzen sie zu Beginn auf 0\n",
        "right_counter, wrong_counter = 0,0\n",
        "\n",
        "# Mit Hilfe der For-Schleife durchlaufen wir alle einzelnen Elemente des Testdatensatzes\n",
        "for j in range(pred.shape[0]):\n",
        "    spalte = X_test.iloc[j].replace(0, float(\"NaN\"))\n",
        "    spalte.dropna(inplace=True)\n",
        "    print(f\"Kategorie: {spalte.index.values[0]}, District: {spalte.index.values[1]}\")\n",
        "    print(f\"Die Vorhersage war: {pred[j]}\")\n",
        "    print(f\"Der richtige Wert war: {y_test.iloc[j]} \\n\")\n",
        "    if (pred[j] == y_test.iloc[j]):\n",
        "        right_counter += 1\n",
        "    else:\n",
        "        wrong_counter += 1"
      ],
      "metadata": {
        "id": "HZ_BlM4Jiaor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Anzahl der richtigen Vorhersagen: {right_counter}, Anzahl der falschen Vorhersagen: {wrong_counter} \\n\")\n",
        "\n",
        "# Berechnen der Genauigkeit des Modells\n",
        "accuracy = right_counter / (right_counter + wrong_counter)\n",
        "print(f\"Genauigkeit des Modells: {round(accuracy*100, 2)}%\")"
      ],
      "metadata": {
        "id": "50-2S1Il5LB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h0aOP72gw6gD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}